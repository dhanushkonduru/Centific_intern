# Centific_intern

# What is Model Inference?

In the context of machine learning and AI, inference refers to the process of using a pre-trained model to make predictions or generate outputs based on new, unseen data.

In this project, inference is performed using powerful open-weight language models like DeepSeek and Microsoft's Phi. These models are capable of understanding prompts and generating highly coherent text, code, or analysis based on context.

Unlike training, where the model learns from data, inference is about applying that learning to real-world inputs. Think of it as the brain of the model being put to use.

